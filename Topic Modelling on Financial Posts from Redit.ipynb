{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Joshuajee/AI-ML-PROJECTS/blob/master/Topic%20Modelling%20on%20Financial%20Posts%20from%20Redit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyLDAvis\n",
        "!pip install bertopic\n",
        "!pip install flair\n",
        "!apt-get -qq install -y libfluidsynth1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XtqmHniDt5H_",
        "outputId": "2f8603db-a9b4-42a6-a535-d0ca02a2b28e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyLDAvis in /usr/local/lib/python3.11/dist-packages (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.24.2 in /usr/local/lib/python3.11/dist-packages (from pyLDAvis) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from pyLDAvis) (1.13.1)\n",
            "Requirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pyLDAvis) (2.2.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from pyLDAvis) (1.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from pyLDAvis) (3.1.6)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.11/dist-packages (from pyLDAvis) (2.10.2)\n",
            "Requirement already satisfied: funcy in /usr/local/lib/python3.11/dist-packages (from pyLDAvis) (2.0)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from pyLDAvis) (1.6.1)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (from pyLDAvis) (4.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from pyLDAvis) (75.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0->pyLDAvis) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0->pyLDAvis) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0->pyLDAvis) (2025.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.0.0->pyLDAvis) (3.6.0)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim->pyLDAvis) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->pyLDAvis) (3.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->pyLDAvis) (1.17.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim->pyLDAvis) (1.17.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-b8JCc6RYng"
      },
      "source": [
        "** **\n",
        "## Step 1: Loading the Data\n",
        "** **\n",
        "The data was collected manually from twenty two financial subreddit and saved in a csv format to my github repo."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "import os\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.metrics.pairwise import cosine_distances\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage\n",
        "\n",
        "# BERTopic model\n",
        "from bertopic import BERTopic\n",
        "# Dimension reduction\n",
        "from umap import UMAP\n",
        "# Clustering\n",
        "from hdbscan import HDBSCAN\n",
        "from sklearn.cluster import KMeans\n",
        "# Count vectorization\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "# Sentence transformer\n",
        "from sentence_transformers import SentenceTransformer\n",
        "# Flair\n",
        "from transformers.pipelines import pipeline\n",
        "from flair.embeddings import TransformerDocumentEmbeddings\n"
      ],
      "metadata": {
        "id": "W0YSxpNufnop"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_redit_data_from_github():\n",
        "  file_path = \"https://raw.githubusercontent.com/Joshuajee/AI-ML-PROJECTS/master/data/reddit/reddit_financial_data.csv\"\n",
        "  reponse = requests.get(file_path)\n",
        "  if reponse.status_code == 200:\n",
        "    with open(\"reddit_financial_data.csv\", \"wb\") as f:\n",
        "      f.write(reponse.content)\n",
        "    return pd.read_csv(\"reddit_financial_data.csv\", sep=\",\")\n",
        "  else:\n",
        "    raise Exception(\"Error downloading\", reponse.status_code)\n"
      ],
      "metadata": {
        "id": "Q75VrnVuhtmp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reddit_data = get_redit_data_from_github()\n",
        "reddit_data"
      ],
      "metadata": {
        "id": "1tg2L1BjiCJ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "** **\n",
        "## Step 2: Data Cleaning\n",
        "** **\n",
        "\n",
        "The reddit post data contains multiple columns, but since this is an NLP task only the text and title columns are useful for our Topic modeling task the other columns will be ignored.\n",
        "\n",
        "1. Join the title and the text columns\n",
        "2. Remove punctuations and special characters.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Zb_9_fxFlDcs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Join the title and text columns in a new content column\n",
        "reddit_data['content'] = reddit_data['title'] + ' ' + reddit_data['text']\n",
        "reddit_data"
      ],
      "metadata": {
        "id": "W-zzxTSBn20F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new DataFrame containing only the content column\n",
        "content_df = reddit_data[['content']].copy()\n",
        "content_df"
      ],
      "metadata": {
        "id": "emUJNjbZqbBf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(text):\n",
        "    text = re.sub(r'http\\S+|www\\S+', '', text)  # Remove URLs\n",
        "    text = re.sub(r'[^A-Za-z0-9\\s]+', '', text)  # Remove special characters\n",
        "    text = text.lower()  # Lowercase text\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra spaces\n",
        "    return text"
      ],
      "metadata": {
        "id": "fr5STEgClB-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Deohesjbt57-"
      },
      "outputs": [],
      "source": [
        "# Cleaning the data gotton from reddit as it contains relevant characters\n",
        "content_df['cleaned_content'] = content_df['content'].apply(preprocess)\n",
        "content_df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing stop words\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('stopwords')\n",
        "stop_words = stopwords.words('english')\n",
        "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('wordnet')\n",
        "wn = nltk.WordNetLemmatizer()\n",
        "\n",
        "# Removing stop words from wordlist columns\n",
        "content_df['cleaned_content_no_sw'] = content_df['cleaned_content'].apply(lambda x: ' '.join([w for w in x.split() if w not in stop_words]))\n",
        "# Lemmatization\n",
        "content_df['cleaned_content_lm_no_sw'] = content_df['cleaned_content_no_sw'].apply(lambda x: ' '.join([wn.lemmatize(w) for w in x.split() if w not in stop_words]))\n",
        "content_df"
      ],
      "metadata": {
        "id": "g9ZjZTUaKs-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "** **\n",
        "## Step 3: Exploratory Analysis <a class=\"anchor\\\" id=\"eda\"></a>\n",
        "** **\n",
        "To better understand our data, I will make and histogram about to show the distrubution of words per posts.\n",
        "\n",
        "To verify whether the preprocessing, weâ€™ll make a simple word cloud using the `wordcloud` package to get a visual representation of most common words. It is key to understanding the data and ensuring we are on the right track, and if any more preprocessing is necessary before training the model.\n",
        "\n"
      ],
      "metadata": {
        "id": "1xKdZLCdrQjt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(content_df['cleaned_content'])\n",
        "print(f\"Total unique words raw               : {len(vectorizer.get_feature_names_out())}\")\n",
        "X = vectorizer.fit_transform(content_df['cleaned_content_no_sw'])\n",
        "print(f\"Total unique words without stop words: {len(vectorizer.get_feature_names_out())}\")\n",
        "X = vectorizer.fit_transform(content_df['cleaned_content_lm_no_sw'])\n",
        "print(f\"Total unique words with lemmatization: {len(vectorizer.get_feature_names_out())}\")"
      ],
      "metadata": {
        "id": "rKWDyECPW6L3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_lengths = [len(x) for x in content_df['cleaned_content_lm_no_sw'].apply(lambda x: [w for w in x.split() if w not in stop_words])]\n",
        "\n",
        "# Set up the figure size\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Plot the histogram using seaborn with a KDE overlay.\n",
        "sns.histplot(text_lengths, bins=50, kde=True, color=\"steelblue\")\n",
        "\n",
        "# Add plot labels and title\n",
        "plt.title(\"Distribution of Text Lengths\")\n",
        "plt.xlabel(\"Text Length (number of words)\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mqT6c6qOIMmJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the wordcloud library\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "data_words = content_df['cleaned_content_lm_no_sw'].explode().to_list()\n",
        "\n",
        "# Create a WordCloud object\n",
        "wordcloud = WordCloud(background_color=\"white\", max_words=1000, contour_width=3, width=1000, height=600, contour_color='steelblue')\n",
        "\n",
        "# Generate a big chunck of text\n",
        "big_chunck_text = \" \".join(data_words)\n",
        "\n",
        "# Generate a word cloud\n",
        "wordcloud.generate(big_chunck_text)\n",
        "\n",
        "# Visualize the word cloud\n",
        "wordcloud.to_image()"
      ],
      "metadata": {
        "id": "nvclVKbJrSu-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "** **\n",
        "## Step 4: Building the Models <a class=\"anchor\\\" id=\"models\"></a>\n",
        "** **\n",
        "\n",
        "For this tasks, I will be using LDA and Bertopic\n"
      ],
      "metadata": {
        "id": "isdz_8dcDXE6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "n_components = 10\n",
        "n_top_words = 10\n",
        "max_top_words = 100"
      ],
      "metadata": {
        "id": "m7O9bEHUNrGw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "** **\n",
        "### Latent Dirichlet Allocation (LDA) <a class=\"anchor\\\" id=\"lda\"></a>\n",
        "** **\n"
      ],
      "metadata": {
        "id": "zv1sKtPoPMWA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters for LDA\n",
        "max_df = 0.9\n",
        "min_df = 4\n",
        "max_features = 10000\n",
        "\n",
        "tf_vectorizer = CountVectorizer(max_df=max_df, min_df=min_df, max_features=max_features, stop_words=\"english\")\n",
        "\n",
        "tf = tf_vectorizer.fit_transform(content_df['cleaned_content_lm_no_sw'])\n",
        "\n",
        "lda_model = LatentDirichletAllocation(n_components=n_components, learning_method=\"online\", random_state=100)\n",
        "\n",
        "lda_model.fit(tf)"
      ],
      "metadata": {
        "id": "TJvHPySaDWR3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "** **\n",
        "###  BERTopic Model <a class=\"anchor\\\" id=\"bert_model\"></a>\n",
        "** **"
      ],
      "metadata": {
        "id": "m859Ax-vvySm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use KMeans with n_clusters=n_components i.e number of topics\n",
        "kmeans_model = KMeans(n_clusters=n_components, random_state=100)\n",
        "# Use UMAP to reduce the dimension of the embeddings\n",
        "# n_components should not be mistaken with number of topics\n",
        "# n_components is the dimension to reduce the vector to\n",
        "# min_dist = 0.5 Controls how tightly UMAP packs points together in the low-dimensional space\n",
        "# metric='cosine'\n",
        "umap_model = UMAP(n_components=n_components, min_dist=0.5, metric='cosine', random_state=100)\n",
        "\n",
        "# Using TransformerDocumentEmbeddings and \"roberta-base\" pretrained model\n",
        "model = TransformerDocumentEmbeddings('roberta-base')\n",
        "\n",
        "topic_model = BERTopic(embedding_model=model, umap_model=umap_model, hdbscan_model=kmeans_model, top_n_words=max_top_words)\n",
        "\n",
        "topics, probs = topic_model.fit_transform(content_df['cleaned_content_lm_no_sw'])"
      ],
      "metadata": {
        "id": "VT0fpxFijXjI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the list of topics\n",
        "topic_model.get_topic_info()"
      ],
      "metadata": {
        "id": "srWEBO2kwwWk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic_model.visualize_topics()"
      ],
      "metadata": {
        "id": "jFxMhoZn0Yv9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "** **\n",
        "## Step 5: Results and Visualization <a class=\"anchor\\\" id=\"results\"></a>\n",
        "** **\n"
      ],
      "metadata": {
        "id": "IYRmBuGmFHgj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_top_words(model, feature_names, n_top_words, title):\n",
        "    fig, axes = plt.subplots(2, 5, figsize=(30, 15), sharex=True)\n",
        "    axes = axes.flatten()\n",
        "    for topic_idx, topic in enumerate(model.components_):\n",
        "        top_features_ind = topic.argsort()[: -n_top_words - 1 : -1]\n",
        "        top_features = [feature_names[i] for i in top_features_ind]\n",
        "        weights = topic[top_features_ind]\n",
        "\n",
        "        ax = axes[topic_idx]\n",
        "        ax.barh(top_features, weights, height=0.7)\n",
        "        ax.set_title(f\"Topic {topic_idx +1}\", fontdict={\"fontsize\": 30})\n",
        "        ax.invert_yaxis()\n",
        "        ax.tick_params(axis=\"both\", which=\"major\", labelsize=20)\n",
        "        for i in \"top right left\".split():\n",
        "            ax.spines[i].set_visible(False)\n",
        "        fig.suptitle(title, fontsize=40)\n",
        "\n",
        "    plt.subplots_adjust(top=0.90, bottom=0.05, wspace=0.90, hspace=0.3)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_dendrogram(topic_word_matrix, name):\n",
        "    dist_matrix = cosine_distances(topic_word_matrix)\n",
        "    linkage_matrix = linkage(dist_matrix, method='ward')\n",
        "    # Plot dendrogram\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    dendrogram(linkage_matrix, labels=[f\"Topic {i + 1}\" for i in n_components])\n",
        "    plt.title(f\"Hierarchical Clustering of {name} Topics\")\n",
        "    plt.xlabel(\"Topic\")\n",
        "    plt.ylabel(\"Cosine Distance\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "HKe6QxmNO17g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Latent Dirichlet Allocation (LDA)"
      ],
      "metadata": {
        "id": "pXPP-QA6Dov2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf_feature_names = tf_vectorizer.get_feature_names_out()\n",
        "plot_top_words(lda_model, tf_feature_names, n_top_words, \"Topics in LDA model\")"
      ],
      "metadata": {
        "id": "RUPVfYSaFKrO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for topic_idx, topic in enumerate(lda_model.components_):\n",
        "    top_features_ind = topic.argsort()[: -max_top_words - 1 : -1]\n",
        "    top_features = [tf_feature_names[i] for i in top_features_ind]\n",
        "    weights = topic[top_features_ind]\n",
        "    word_freq = dict()\n",
        "    for i in range(len(top_features)):\n",
        "        word_freq[top_features[i]] = weights[i]\n",
        "    wordcloud = WordCloud().generate_from_frequencies(word_freq)\n",
        "    plt.clf()\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.axis('off')\n",
        "    plt.title(f\"LDA Topic {topic_idx + 1}\", fontsize=14)\n",
        "    plt.show()\n",
        "    print(\"\")"
      ],
      "metadata": {
        "id": "JVSMdXHN3QKa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic_word_matrix = lda_model.components_ / lda_model.components_.sum(axis=1)[:, np.newaxis]\n",
        "plot_dendrogram(topic_word_matrix, \"Latent Dirichlet Allocation (LDA)\")"
      ],
      "metadata": {
        "id": "_vuBZooLIEvy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BERTopic"
      ],
      "metadata": {
        "id": "SjRJyzf9HU-5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming topic_model is your BERTopic model\n",
        "topics = topic_model.get_topics()\n",
        "\n",
        "for topic_num in topics:\n",
        "    if topic_num == -1:  # Skip the outlier/no-topic category\n",
        "        continue\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    word_freq = dict(topics[topic_num])\n",
        "    print(\"\")\n",
        "    wordcloud = WordCloud().generate_from_frequencies(word_freq)\n",
        "    plt.clf()\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.axis('off')\n",
        "    plt.title(f\"BERTopic Topic {topic_num + 1}\", fontsize=14)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "l4HGT6Oqy3i-"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}